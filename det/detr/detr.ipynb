{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up variables and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"microsoft/conditional-detr-resnet-50\"\n",
    "IMAGE_SIZE = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate timm\n",
    "!pip install -q -U albumentations>=1.4.5 torchmetrics pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing and playing with data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cppe5 = load_dataset('cppe-5')\n",
    "\n",
    "print(cppe5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in cppe5:\n",
    "  split = cppe5[\"train\"].train_test_split(0.15, seed = 42)\n",
    "  cppe5[\"train\"] = split[\"train\"]\n",
    "  cppe5[\"validation\"] = split[\"test\"]\n",
    "\n",
    "cppe5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "image = cppe5[\"train\"][2][\"image\"]\n",
    "annotations = cppe5[\"train\"][2][\"objects\"]\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "for i in range(len(annotations[\"id\"])):\n",
    "    box = annotations[\"bbox\"][i]\n",
    "    class_idx = annotations[\"category\"][i]\n",
    "    x, y, w, h = tuple(box)\n",
    "    # Check if coordinates are normalized or not\n",
    "    if max(box) > 1.0:\n",
    "        # Coordinates are un-normalized, no need to re-scale them\n",
    "        x1, y1 = int(x), int(y)\n",
    "        x2, y2 = int(x + w), int(y + h)\n",
    "    else:\n",
    "        # Coordinates are normalized, re-scale them\n",
    "        x1 = int(x * width)\n",
    "        y1 = int(y * height)\n",
    "        x2 = int((x + w) * width)\n",
    "        y2 = int((y + h) * height)\n",
    "    draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "MAX_SIZE = IMAGE_SIZE\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    do_resize=True,\n",
    "    size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE},\n",
    "    do_pad=True,\n",
    "    pad_size={\"height\": MAX_SIZE, \"width\": MAX_SIZE},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "* When we augment images for detection purposes, we need to make sure that bounding box coordinates are augmented too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface's albumentations helps that case:\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "train_augment_and_transform = A.Compose(\n",
    "    [\n",
    "        A.Perspective(p=0.1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.HueSaturationValue(p=0.1),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n",
    ")\n",
    "\n",
    "validation_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n",
    "    \"\"\"Format one set of image annotations to the COCO format\n",
    "\n",
    "    Args:\n",
    "        image_id (str): image id. e.g. \"0001\"\n",
    "        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n",
    "        areas (List[float]): list of corresponding areas to provided bounding boxes\n",
    "        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n",
    "            ([center_x, center_y, width, height] in absolute coordinates)\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"image_id\": image id,\n",
    "            \"annotations\": list of formatted annotations\n",
    "        }\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    for category, area, bbox in zip(categories, areas, bboxes):\n",
    "        formatted_annotation = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category,\n",
    "            \"iscrowd\": 0,\n",
    "            \"area\": area,\n",
    "            \"bbox\": list(bbox),\n",
    "        }\n",
    "        annotations.append(formatted_annotation)\n",
    "\n",
    "    return {\n",
    "        \"image_id\": image_id,\n",
    "        \"annotations\": annotations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n",
    "    \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "        images.append(output[\"image\"])\n",
    "\n",
    "        formatted_annotations = format_image_annotations_as_coco(\n",
    "            image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n",
    "        )\n",
    "        annotations.append(formatted_annotations)\n",
    "\n",
    "    result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "\n",
    "    if not return_pixel_mask:\n",
    "        result.pop(\"pixel_mask\", None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Make transform functions for batch and apply for dataset splits\n",
    "train_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n",
    ")\n",
    "validation_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n",
    ")\n",
    "\n",
    "cppe5[\"train\"] = cppe5[\"train\"].with_transform(train_transform_batch)\n",
    "cppe5[\"validation\"] = cppe5[\"validation\"].with_transform(validation_transform_batch)\n",
    "cppe5[\"test\"] = cppe5[\"test\"].with_transform(validation_transform_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cppe5[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have successfully augmented the individual images and prepared their annotations. However, preprocessing isnâ€™t complete yet. In the final step, create a custom collate_fn to batch images together. Pad images (which are now pixel_values) to the largest image in a batch, and create a corresponding pixel_mask to indicate which pixels are real (1) and which are padding (0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing evaluation metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n",
    "    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): Bounding boxes in YOLO format\n",
    "        image_size (Tuple[int, int]): Image size in format (height, width)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # convert center to corners format\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    height,width = image_size\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "    # multiplies each boxâ€™s normalized values by the actual image width and height to convert them to absolute coordinates.\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
    "  \"\"\"\n",
    "    Compute mean average mAP, mAR and their variants for the object detection task.\n",
    "\n",
    "    Args:\n",
    "        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
    "        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n",
    "        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n",
    "  \"\"\"\n",
    "\n",
    "  predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "    # For metric computation we need to provide:\n",
    "    #  - targets in a form of list of dictionaries with keys \"boxes\", \"labels\"\n",
    "    #  - predictions in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n",
    "\n",
    "  image_sizes = []\n",
    "  post_processed_targets = []\n",
    "  post_processed_predictions = []\n",
    "  for batch in targets:\n",
    "        # collect image sizes, we will need them for predictions post processing\n",
    "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
    "        image_sizes.append(batch_image_sizes)\n",
    "        # collect targets in the required format for metric computation\n",
    "        # boxes were converted to YOLO format needed for model training\n",
    "        # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "        for image_target in batch:\n",
    "            boxes = torch.tensor(image_target[\"boxes\"])\n",
    "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
    "            labels = torch.tensor(image_target[\"class_labels\"])\n",
    "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "  # Collect predictions in the required format for metric computation,\n",
    "  # model produce boxes in YOLO format, then image_processor convert them to Pascal VOC format\n",
    "    \n",
    "  for batch, target_sizes in zip(predictions, image_sizes):\n",
    "        batch_logits, batch_boxes = batch[1], batch[2]\n",
    "        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "        post_processed_output = image_processor.post_process_object_detection(\n",
    "            output, threshold=threshold, target_sizes=target_sizes\n",
    "        )\n",
    "        post_processed_predictions.extend(post_processed_output)\n",
    "\n",
    "  # Compute metrics\n",
    "  metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "  metric.update(post_processed_predictions, post_processed_targets)\n",
    "  metrics = metric.compute()\n",
    "\n",
    "  # Replace list of per class metrics with separate metric for each class\n",
    "  classes = metrics.pop(\"classes\")\n",
    "  map_per_class = metrics.pop(\"map_per_class\")\n",
    "  mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "  for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "      class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "      metrics[f\"map_{class_name}\"] = class_map\n",
    "      metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "  metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "  return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_compute_metrics_fn = partial(\n",
    "    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
